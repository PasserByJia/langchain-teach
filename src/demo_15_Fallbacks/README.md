# 演示 15: 回退机制

本演示展示了如何在 LangChain 表达式语言 (LCEL) 中使用 `.withFallbacks()` 来构建具有弹性、可容错的语言模型应用。当主流程 (Runnable) 失败时，可以平滑地切换到备用流程，从而提高应用的稳定性和可靠性。

本 `demo` 包含了四个核心示例，覆盖了 `fallbacks` 的主要应用场景。

## 功能演示

`index.ts` 文件中的代码演示了以下四种 `fallback` 机制：

1.  **处理 LLM API 错误**: 这是最常见的用例。当主 LLM 服务因故（如 API 宕机、速率限制）失败时，自动切换到备用的 LLM 服务。示例中，我们尝试使用一个不存在的 `potato!` 模型，当调用失败后，会自动切换到 `ChatAnthropic` 模型并成功返回结果。

2.  **为 RunnableSequences 设置回退**: 回退机制不仅限于单个 LLM，还可以应用于整个链 (RunnableSequence)。当一个链因为模型或提示词不兼容而失败时，可以切换到另一个使用不同模型和提示词的备用链。示例中，`badChain` 因为使用了无效模型而失败，进而触发 `goodChain`，它使用了不同的提示词和模型来完成任务。

3.  **处理超长输入**: LLM 的一个主要限制是其上下文窗口大小。当输入内容过长时，调用会失败。通过设置 `fallback`，我们可以尝试使用一个上下文窗口较小的模型（如 `gpt-3.5-turbo`），如果失败，则自动切换到一个上下文窗口更大的模型（如 `gpt-3.5-turbo-16k`）。

4.  **回退到更强大的模型以保证输出格式**: 某些场景下，我们需要模型返回特定格式的输出（如 JSON）。一些轻量级模型可能无法稳定地满足这一要求。我们可以先尝试使用一个更便宜、更快的模型，如果它返回的格式不正确导致解析失败，再 `fallback` 到一个更强大、更可靠的模型（如 `gpt-4`）来确保输出的正确性。

## 如何运行

1.  确保你已经安装了所有必要的依赖：

    ```bash
    npm install @langchain/openai @langchain/anthropic zod
    ```

2.  在项目根目录下创建一个 `.env` 文件，并填入你的 OpenAI 和 Anthropic API 密钥：

    ```
    OPENAI_API_KEY=sk-...
    ANTHROPIC_API_KEY=sk-...
    ```

3.  运行 `index.ts` 文件：

    ```bash
    npx ts-node src/demo_15_Fallbacks/index.ts
    ```

## 适用场景

-   **提高生产环境应用的可靠性**: 在生产环境中，任何对外部服务的调用都可能失败。通过为关键的 LLM 调用链设置 `fallback`，可以确保即使主服务出现问题，你的应用依然能够正常响应，从而避免服务中断。

-   **成本与性能的平衡**: 你可以默认使用一个速度快、成本低的 LLM 模型来处理大部分请求。对于那些该模型无法处理的复杂请求（例如，需要更强推理能力或特定格式输出），则可以 `fallback` 到一个更强大但更昂贵的模型。这样可以在保证服务质量的同时，有效控制成本。

-   **处理不同模型的兼容性问题**: 不同的 LLM 可能需要不同格式的提示词。当需要从一个模型切换到另一个时，`fallback` 机制允许你不仅切换模型，还切换整个处理链（包括提示词模板），从而确保兼容性。

-   **动态适应输入负载**: 对于输入长度不定的应用，`fallback` 提供了一种优雅的方式来处理超出上下文窗口的请求，而无需在调用前进行复杂的长度计算和切分。这简化了代码逻辑，并提高了应用的鲁棒性。